\documentclass{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\betahat{{\widehat\beta}}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_knit$set(root.dir= "../")
opts_chunk$set(cache.path='cache/', fig.path='figures/', fig.align='center' )
@

\section{Notation}

Let $C$ be a set of observed covariates, $A$ be an indicator of treatment, and $Y_a$ be the outcome observed under $A=a$. The observed outcome is given by $Y = AY_1 + (1-A)Y_0$. 

We are interested in comparing features of the marginal distributions of the potential outcomes. Let $f_{Y_a}$ denote the marginal distribution of potential outcome $Y_a$ (with CDF given by $F_{Y_a}$). We are interested in features of these distributions, which we will either describe through the quantiles or the mean. 

\begin{align*}
  E(Y_a) &= \int y f_{Y_a}(y) \partial y \\
  Q_{Y_a}(\tau) &= F^{-1}_{Y_a}(\tau)
\end{align*}

Examples of comparisons are the average causal effect (ATE), $E(Y_1-Y_0)$, the median causal effect, $Q_{Y_1}(0.5)-Q_{Y_0}(0.5)$, and more generally, the $p^{th}$ quantile causal effect, $Q_{Y_1}(p)-Q_{Y_0}(p)$.

A few notes. First, the expected value is the integral of the quantile function:

\begin{align*}
  E(Y_a) &= \int_0^1 Q_{Y_a}(\tau) \partial \tau
\end{align*}

\noindent Second, if we view the quantile causal effect as a function, then we can consider estimating the quantile effect for all $p$:

\begin{align*}
  QE(\tau) &= Q_{Y_1}(\tau)-Q_{Y_0}(\tau)
\end{align*}

\noindent Finally, combining the first two notes, we get that the ATE is simply the quantile effect integrated over $\tau$:

\begin{align*}
  ATE &= E(Y_1-Y_0) = \int_0^1 QE(\tau) \partial \tau
\end{align*}

\section{Main assumptions}

The primary assumption will be strong ignorability, 

\begin{align*}
  (Y_1,Y_0) \bot A | C
\end{align*}

\noindent Based on Rubin, we know that this implies 

\begin{align*}
  (Y_1,Y_0) \bot A | e(C)
\end{align*}

\noindent where $e(C)=P(A=1|C)$ is known as the propensity score. The implication of strong ignorability is that the potential outcomes are independent of the treatment assignment conditional on the propensity score. That is, conditional on the propensity score, we can ignore the treatment assignment mechanism when describing features of the potential outcomes. In most applications of causal inference, we focus on the ATE. However, strong ignorability makes a statement about the entire distribution of the potential outcomes, so that we can make comparisons between any feature of the potential outcomes of interest (e.g. quantiles). 

There is stuff out there about local ignorability, or something like that. I've seen it in the context of regression discontinuity designs. Probably should have a look. 

We will also need SUTVA. No big deal. 

\section{Local quantile regression}

We will use local quantile regression to estimate $QE(p)$ around a given value of the propensity score. In other words, we will estimate the conditional quantile effect $QE(p|e(C)=\psi)$ using local quantile regression, where locality is based on the propensity score.

For a given value of the propensity score $\psi$, we seek to minimize

\begin{align*}
  \sum_i w_i \rho_\tau( Y_i - f(X_i))
\end{align*}

where $w_i$ is a weight and $\rho_\tau(u)=u(\tau-I(u<0))$. We choose the weight based on a kernel function, $K(.)$, and the bandwidth, $h$. Specifically, $w_i=K( \frac{\psi-e(C_i)}{h})$. Since the kernel function weights more heavily the observations that have propensity score close to $\psi$, we are estimating a local (in the PS) quantile effect. I'm sure we could somehow modify strong ignorability to explicity take into account the localness of the procedure. 

So we have the $\tau^{th}$ quantile effect around $e(C)=\psi$

\begin{align*}
  \betahat_\tau(\psi) = \widehat{QE}(p|e(C)=\psi) = \argmin_\beta \sum_i K\left( \frac{\psi-e(C_i)}{h}\right) \rho_\tau( Y_i - \beta_0 - \beta_1 A_i)
\end{align*}

I'll also note that it is possible to do smoothing the the quantile direction using similar weights. The goal there would be to borrow information across quantiles to increase the smoothness of the effects as a function of the quantile. Implementation would probably be much more challenging.

\subsection{Conditional ATE}

Let the conditional ATE be given by $ \beta(\psi) = E(Y_1-Y_0|e(C)=\psi)$. Using relationship between the quantiles and mean above, we can write the conditional ATE as:

\begin{align*}
   \beta(\psi) &= \int_0^1 \beta_\tau(\psi) \partial \tau
\end{align*}

Since we are estimating $\beta_\tau(\psi)$, we can easily estimate $\beta(\psi)$:

\begin{align*}
   \betahat(\psi) &= \int_0^1 \betahat_\tau(\psi) \partial \tau
\end{align*}

\subsection{ATE, ATT, or any other marginal effect of interest}

Let the ATE be given by $\beta = E(Y_1-Y_0)$. By integrating the conditional ATE over the propensity score distribution, we can write $\beta$ as:

\begin{align*}
   \beta &= \int_0^1 \beta(\psi) f_{e(C)}(\psi) \partial \psi
\end{align*}

which provides the readily available estimate:

\begin{align*}
   \widehat{\beta} &= \int_0^1 \betahat(\psi) \widehat{f}_{e(C)}(\psi) \partial \psi
\end{align*}

ATT, or any other marginal effect, can be estimated by replacing the distribution $f_{e(C)}$ that we are marginalizing over with the appropriate distribution. 

\subsection{Estimating marginal quantile effects}

This is wrong. Let the marginal quantile be given by $ \beta_\tau = \int_0^1 QE(p|e(C)=\psi) f_{e(C)}(\psi) \partial \psi$. 

\begin{align*}
   \betahat_\tau &=  \int_0^1 \betahat_\tau(\psi) \widehat{f}_{e(C)}(\psi) \partial \psi \\
   \widehat{f}_{e(C)}(x) &= \frac{1}{nh} \sum_{i=1}^n K\left( \frac{x-e(C_i)}{h}\right)
\end{align*}


\section{R code example}

First, we fit the model and suppress all output. So we don't see anything here!

<<lalonde-fit, eval=TRUE, echo=FALSE, include=FALSE, cache=TRUE >>=
source("code/examples/lalonde.r")
#print("Comment out")
@

Next, we run the code to produce the heatmaps!
<<lalonde-heatmap, eval=TRUE, echo=TRUE , cache=TRUE , dependson="lalonde-fit" >>=
heatmap.localQ(fit=out , Ytype="quantile" , pch=16 , xlab="PS")
@

Next, we run the code to produce the mean plot!
<<lalonde-meanplot, eval=TRUE, echo=TRUE , cache=TRUE , dependson="lalonde-fit" >>=
plot.Qmean(fit=out , add=F , col='blue' , xlab="PS" , ylab="Effect")
@

Next, we run the code to produce the quantile plot!
<<lalonde-Qplot, eval=TRUE, echo=TRUE , cache=TRUE , dependson="lalonde-fit" >>=
plot.Qavg(fit=out , Ytype="quantile" , type='l' , col='blue' , xlab="quantile of Y(0)" , ylab="Effect")
@


\section{Substance abuse example from Beth Ann}

<<chestnut-fit, eval=TRUE, echo=FALSE, include=FALSE, cache=TRUE >>=
load("data/results/chesnut_results.Rdata")
#print("Comment out")
@

\subsection{SFS}
<<SFS-heatmap, eval=TRUE, echo=TRUE , cache=TRUE , dependson="chestnut-fit" >>=
heatmap.localQ(fit=out.sfs , Ytype="quantile" , pch=16 , xlab="PS")
plot.Qmean(fit=out.sfs , add=F , col='blue' , xlab="PS" , ylab="Effect" , boot=T)
plot.Qavg(fit=out.sfs  , type='l' , col='blue' , xlab="quantile of Y(0)" , ylab="Effect")
@
\subsection{SPS}
<<SPS-heatmap, eval=TRUE, echo=TRUE , cache=TRUE , dependson="chestnut-fit" >>=
#heatmap.localQ(fit=out.sps , Ytype="quantile" , pch=16 , xlab="PS")
#plot.Qmean(fit=out.sps , add=F , col='blue' , xlab="PS" , ylab="Effect")
#plot.Qavg(fit=out.sps  , type='l' , col='blue' , xlab="quantile of Y(0)" , ylab="Effect")
@
\subsection{EPS}
<<EPS-heatmap, eval=TRUE, echo=TRUE , cache=TRUE , dependson="chestnut-fit" >>=
#heatmap.localQ(fit=out.eps , Ytype="quantile" , pch=16 , xlab="PS")
#plot.Qmean(fit=out.eps , add=F , col='blue' , xlab="PS" , ylab="Effect")
#plot.Qavg(fit=out.eps  , type='l' , col='blue' , xlab="quantile of Y(0)" , ylab="Effect")
@
\subsection{SDS}
<<SDS-heatmap, eval=TRUE, echo=TRUE , cache=TRUE , dependson="chestnut-fit" >>=
#heatmap.localQ(fit=out.sds , Ytype="quantile" , pch=16 , xlab="PS")
#plot.Qmean(fit=out.sds , add=F , col='blue' , xlab="PS" , ylab="Effect")
#plot.Qavg(fit=out.sds  , type='l' , col='blue' , xlab="quantile of Y(0)" , ylab="Effect")
@

\end{document}


